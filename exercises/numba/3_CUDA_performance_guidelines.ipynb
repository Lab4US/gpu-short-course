{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outer-promise",
   "metadata": {
    "id": "outer-promise"
   },
   "source": [
    "# Lecture 3. Performance guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J7GiIP54uVnu",
   "metadata": {
    "id": "J7GiIP54uVnu"
   },
   "source": [
    "In this notebook we will describe the most important guidelines when programming code for NVIDIA GPU cards.\n",
    "\n",
    "We will consider the following aspects of optimizing CUDA kernels:\n",
    "- memory access patterns: *memory coalescing* for the best throughput,\n",
    "- control flow: how code branching affects the performance,\n",
    "- multiprocessor occupancy: experimenting with different block sizes,\n",
    "- instruction-level optimizations: avoiding data type conversion, using floating point *intrisincs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Iavk9Cs4i_Ic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1721,
     "status": "ok",
     "timestamp": 1624793206766,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "Iavk9Cs4i_Ic",
    "outputId": "abc29480-3c64-4cdf-c5c7-263ca27ac732"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/us4useu/ius-2021-gpu-short-course.git\n",
      "  Cloning https://github.com/us4useu/ius-2021-gpu-short-course.git to /tmp/pip-req-build-wbr_nmpi\n",
      "  Running command git clone -q https://github.com/us4useu/ius-2021-gpu-short-course.git /tmp/pip-req-build-wbr_nmpi\n",
      "Building wheels for collected packages: gpu-short-course\n",
      "  Building wheel for gpu-short-course (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gpu-short-course: filename=gpu_short_course-0.0.1-py3-none-any.whl size=3119 sha256=c75a350312997dad55e8fdbbd05d52f80052a9f42ac13d200433f0061bfbd6dd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fahb98fv/wheels/2e/0b/02/0f96bd15166b9aa28455baa3c75c129be82031e9b820986f2d\n",
      "Successfully built gpu-short-course\n",
      "Installing collected packages: gpu-short-course\n",
      "  Attempting uninstall: gpu-short-course\n",
      "    Found existing installation: gpu-short-course 0.0.1\n",
      "    Uninstalling gpu-short-course-0.0.1:\n",
      "      Successfully uninstalled gpu-short-course-0.0.1\n",
      "Successfully installed gpu-short-course-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --force-reinstall git+https://github.com/us4useu/ius-2021-gpu-short-course.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K7TzNH4sUfFd",
   "metadata": {
    "id": "K7TzNH4sUfFd"
   },
   "source": [
    "## Exercise 3.1. Memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q2u5NmF997ls",
   "metadata": {
    "id": "Q2u5NmF997ls"
   },
   "source": [
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.\n",
    "\n",
    "This means that the number of useful memory accesses done by our kernel, and thus its performance, largely depends on the memory access pattern it does.\n",
    "\n",
    "Our goal is to implement a GPU kernel that only loads **useful** data from global memory that will then be used in the calculations. We can achieve this with **coalesced memory accesses**.\n",
    "\n",
    "To achive coalesced memory accesses in our kernel, we need to meet the following conditions:\n",
    "- the number of threads per block is a multiple of 32 threads,\n",
    "- sequential threads in a warp access memory that is sequential.\n",
    "\n",
    "\n",
    "For example, our baseline `add_vectors_gpu` and `convolve_gpu` implementations satisfy the above conditions:\n",
    "- the number of threads per block was equal 256,\n",
    "- adjacent threads were reading adjacent memory areas, e.g. thread `i` read the `a[i]` and `b[i]`, and thread `i+1` read `a[i+1]` and `b[i+1]`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hVK75cukFY1P",
   "metadata": {
    "id": "hVK75cukFY1P"
   },
   "source": [
    "We will discuss below what are the reasons for both of the conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pn3fT7ViAlSV",
   "metadata": {
    "id": "pn3fT7ViAlSV"
   },
   "source": [
    "### Exercise 3.1.1. Impact of misaligned accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7g2ZEr-MERds",
   "metadata": {
    "id": "7g2ZEr-MERds"
   },
   "source": [
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qmYr7eNmEpn7",
   "metadata": {
    "id": "qmYr7eNmEpn7"
   },
   "source": [
    "Recall that warp reads global memory by using a sequence of **32-byte** segments transactions.\n",
    "\n",
    "Note that, according to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> Memory allocated through the CUDA Runtime API (...) is guaranteed to be aligned to at least 256 bytes.\n",
    "\n",
    "This means that if the block size is a multiple of the warp size, each block will load only its own data chunk from memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2CgD_Nl-ONbm",
   "metadata": {
    "id": "2CgD_Nl-ONbm"
   },
   "source": [
    "#### Example\n",
    "\n",
    "As an example, we will consider here:\n",
    "- `add_vectors_gpu` function,\n",
    "- block size = 13. \n",
    "\n",
    "Now let's take a look what memory accesses will be performed by thread block 0, 1, etc.\n",
    "\n",
    "**Block 0**\n",
    "\n",
    "- reads memory area [0, 52), size: 52 bytes (13 x 4-byte floats)\n",
    "\n",
    "```\n",
    "[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n",
    "[       block 0 data (52 bytes)       ]\n",
    "```\n",
    "\n",
    "- This requires 2 x 32-byte transfers. \n",
    "- However only 52 bytes will be used (81%). \n",
    "\n",
    "**Block 1**\n",
    "\n",
    "- Reads memory area [52, 104), size: 52 bytes.\n",
    "```\n",
    "[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n",
    "                                       [       block 1 data (52 bytes)       ]\n",
    "```\n",
    "- This requires 3 x 32-byte transfers.\n",
    "- However only 52 bytes will be used (54%). \n",
    "\n",
    "\n",
    "**And so on...**\n",
    "\n",
    "\n",
    "As we can see, we are transfer a large amount of (theoretically) useless data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moOL-8cYJT12",
   "metadata": {
    "id": "moOL-8cYJT12"
   },
   "source": [
    "Let's see if there is any observable performance difference between scripts using 256 and 261 threads in a **block**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qrPeRyL4hIRu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1624793206802,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "qrPeRyL4hIRu",
    "outputId": "faf4d8fe-e24e-4248-b416-0dc718771bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_1_1_aligned.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_1_aligned.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rkdpd78-iBQ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4045,
     "status": "ok",
     "timestamp": 1624793210846,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "rkdpd78-iBQ6",
    "outputId": "5348af40-01ae-4be3-c544-34c6394e274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7062== NVPROF is profiling process 7062, command: python 3_1_1_aligned.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0151 seconds (+/- 0.0345), median: 0.0115\n",
      "==7062== Profiling application: python 3_1_1_aligned.py\n",
      "==7062== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   57.19%  460.05ms       300  1.5335ms  1.2867ms  2.7827ms  [CUDA memcpy DtoH]\n",
      "                   39.68%  319.20ms       200  1.5960ms  1.4214ms  2.6505ms  [CUDA memcpy HtoD]\n",
      "                    3.12%  25.116ms       100  251.16us  248.04us  273.07us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_1_aligned.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yZNAOdG8h-xB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1624793210895,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "yZNAOdG8h-xB",
    "outputId": "5f0980cd-902e-4d75-fecb-45f8af3e07d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_1_1_misaligned.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_1_misaligned.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 261\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "G5TINGpYldmo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1624793214953,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "G5TINGpYldmo",
    "outputId": "7f5cb1d0-006d-44a5-a63d-78d75caab994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7320== NVPROF is profiling process 7320, command: python 3_1_1_misaligned.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0154 seconds (+/- 0.0427), median: 0.0104\n",
      "==7320== Profiling application: python 3_1_1_misaligned.py\n",
      "==7320== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   52.98%  414.09ms       300  1.3803ms  1.2899ms  2.8600ms  [CUDA memcpy DtoH]\n",
      "                   43.77%  342.10ms       200  1.7105ms  1.3977ms  5.3995ms  [CUDA memcpy HtoD]\n",
      "                    3.24%  25.359ms       100  253.59us  250.92us  274.86us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_1_misaligned.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kdy0Y21Tlvij",
   "metadata": {
    "id": "Kdy0Y21Tlvij"
   },
   "source": [
    "The difference in performance of the aligned and misaligned versions is rather minor (on some devices even negligible).\n",
    "\n",
    "Why? \n",
    "\n",
    "In this particular case, adjacent warps **reuse the cached data** their neighbors fetched. \n",
    "\n",
    "Anyway, setting a block size to a multiple of warp size, might be a good rule of thumb: it facilitates coalescing, and (as we will discuss later), helps to avoid wasting multiprocessor computation time on under-populated warps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WVH-4tO1ApIv",
   "metadata": {
    "id": "WVH-4tO1ApIv"
   },
   "source": [
    "### Exercise 3.1.2. Impact of strided accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QAD5qaCi8yhV",
   "metadata": {
    "id": "QAD5qaCi8yhV"
   },
   "source": [
    "A non-unit-strided global memory accesses may impact effective memory bandwidth. \n",
    "\n",
    "We say that GPU kernel performs a unit-strided memory access, if threads with successive identifiers read the data from successive memory areas, in other words, the following access pattern is respected:\n",
    "\n",
    "```\n",
    "x = data[(some custom offset) + threadIdx.x]\n",
    "```\n",
    "\n",
    "When using the data access notation for a multidimensional array, make sure that the last axis is addressed using `threadIdx.x`:\n",
    "\n",
    "```\n",
    "x = data[(other dimensions...), threadIdx.x]\n",
    "```\n",
    "\n",
    "\n",
    "The degradation in performance can be especially apparent when working with multidimensional arrays - the choice of the axis, along which choosing a specific operation is performed, can affect effective bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urpbVefb9WPH",
   "metadata": {
    "id": "urpbVefb9WPH"
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-activation",
   "metadata": {
    "id": "driving-activation"
   },
   "source": [
    "Let's consider a 1D convolution along one of the axes of a 2D array.\n",
    "\n",
    "```\n",
    "         axis 1\n",
    "     ---------------\n",
    "x = [[0,  1,  2,  3], |\n",
    "     [4,  5,  6,  7], | axis 0\n",
    "     [8,  9, 10, 11]] |\n",
    "\n",
    "h = [1, 1]\n",
    "\n",
    "```\n",
    "\n",
    "NumPy stores arrays in row-major order, so the above array is actually kept in computer's memory as a following 1D array:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-packaging",
   "metadata": {
    "id": "possible-packaging"
   },
   "source": [
    "Let's consider doing convolution along axis 0 and 1.\n",
    "\n",
    "**Convolve along axis 1**:\n",
    "\n",
    "```\n",
    "         axis 1\n",
    "     ---------------\n",
    "x = [[0,  1,  2,  3]  *  [1, 1] = [0,  1,  3,  5] \n",
    "     [4,  5,  6,  7], *  [1, 1] = [4,  9, 11, 13]\n",
    "     [8,  9, 10, 11]] *  [1, 1] = [8, 17, 19, 21]\n",
    "```\n",
    "\n",
    "For the first output row:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n",
    "    [1]      y = [0]\n",
    "    [1, 1]       [1]\n",
    "       [1, 1]    [3]\n",
    "          [1, 1] [5]\n",
    "\n",
    "```\n",
    "\n",
    "`y[0]` `y[1]`, `y[2]` and `y[3]` are computed by threads with `threadIdx.x` equal `0`, `1`, `2` and `3`, respectively.\n",
    "\n",
    "**Convolve along axis 0**:\n",
    "\n",
    "```\n",
    "x = [[0,  1,   2,   3]  | \n",
    "     [4,  5,   6,   7], | axis 0\n",
    "     [8,  9,  10,  11]] | \n",
    "      *   *    *    *\n",
    "     [1] [1]  [1]  [1]\n",
    "     [1] [1]  [1]  [1]\n",
    "   y  =   =    =    =\n",
    "    [ 0] [ 1] [ 2] [ 3]\n",
    "    [ 4] [ 6] [ 8] [10]\n",
    "    [12] [14] [16] [18]\n",
    "```\n",
    "\n",
    "For the first output column:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n",
    "    [1]                                    y = [ 0]\n",
    "    [1,          1]                            [ 4]\n",
    "                [1,         1]                 [12]\n",
    "```\n",
    "\n",
    "`y[0]` `y[1]`, and `y[2]` are computed by threads with `threadIdx.x` equal `0`, `1` and `2` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qsmm48sLaOFB",
   "metadata": {
    "id": "qsmm48sLaOFB"
   },
   "source": [
    "As we can see in the above example, the stride is much larger for the convolution along axis 0. Will it impact the bandwidth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adequate-experiment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1624793215022,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "adequate-experiment",
    "outputId": "b77422d7-92af-4e7a-bd05-b2288af55e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_1_2_convolve_strided_access.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_2_convolve_strided_access.py\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import cupy as cp\n",
    "import gpu_short_course\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_axis0_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    N = len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    HEIGHT = x.shape[0]\n",
    "    WIDTH = x.shape[1]\n",
    "    if i >= HEIGHT or j >= WIDTH:\n",
    "        return\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for k in range(N):\n",
    "        l = i + o - k\n",
    "        if l >= 0 and l < HEIGHT:\n",
    "\n",
    "            ## --- Get data along the second (1) axis.\n",
    "            value += x[l, j] * h[k]\n",
    "            \n",
    "    y[i, j] = value\n",
    "    \n",
    "    \n",
    "def convolve_axis0_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block = (32, 32)\n",
    "    height, width = x.shape\n",
    "    block_h, block_w = block\n",
    "    grid = (math.ceil(width/block_w), \n",
    "            math.ceil(height/block_h))\n",
    "    convolve_axis0_gpu_kernel[grid, block](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_axis1_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    N = len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    \n",
    "    HEIGHT = x.shape[0]\n",
    "    WIDTH = x.shape[1]\n",
    "    \n",
    "    if i >= WIDTH or j >= HEIGHT:\n",
    "        return\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for k in range(N):\n",
    "        l = i + o - k\n",
    "        if l >= 0 and l < WIDTH:\n",
    "            \n",
    "            ## --- Get data along the first (0) axis.\n",
    "            value += x[j, l]*h[k]\n",
    "            \n",
    "    y[j, i] = value\n",
    "    \n",
    "    \n",
    "def convolve_axis1_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block = (32, 32)\n",
    "    height, width = x.shape\n",
    "    block_h, block_w = block\n",
    "    grid = (math.ceil(width/block_w), \n",
    "            math.ceil(height/block_h))\n",
    "    convolve_axis1_gpu_kernel[grid, block](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.run_convolve_2d_input(convolve_axis0_gpu, axis=0)\n",
    "gpu_short_course.run_convolve_2d_input(convolve_axis1_gpu, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rising-tuning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1624793216189,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "rising-tuning",
    "outputId": "4c74105d-4aab-418c-a095-a948386b1fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0: b'GeForce MX250'\n",
      "All tests passed.\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "! python 3_1_2_convolve_strided_access.py --mode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31NCkiJAy0OJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14117,
     "status": "ok",
     "timestamp": 1624793230308,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "31NCkiJAy0OJ",
    "outputId": "d1eb0750-a9f8-47f4-96c1-627475843530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7372== NVPROF is profiling process 7372, command: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking, please wait...\n",
      "Benchmarking, please wait...\n",
      "==7372== Profiling application: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n",
      "==7372== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   61.20%  5.56611s       100  55.661ms  51.235ms  84.812ms  cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   29.55%  2.68800s       100  26.880ms  25.369ms  38.138ms  cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                    5.88%  534.98ms       600  891.63us  1.0880us  2.9331ms  [CUDA memcpy DtoH]\n",
      "                    3.36%  305.87ms       400  764.67us     896ns  3.2279ms  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CTgHQiOX1-vG",
   "metadata": {
    "id": "CTgHQiOX1-vG"
   },
   "source": [
    "On my GPU (Nvidia GeForce MX250), convolution along axis 1 takes much less time than along axis 0.\n",
    "\n",
    "We can use profiler metrics to verify what memory access efficiency for both cases we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "civilian-constant",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6289,
     "status": "ok",
     "timestamp": 1624793236614,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "civilian-constant",
    "outputId": "31917e38-75cb-4083-a866-1e82a46a3cdf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking, please wait...\n",
      "Benchmarking, please wait...\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce MX250 (0)\"\n",
      "    Kernel: cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "          1                            gld_efficiency             Global Memory Load Efficiency      12.50%      12.50%      12.50%\n",
      "          1                            gst_efficiency            Global Memory Store Efficiency      12.50%      12.50%      12.50%\n",
      "    Kernel: cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "          1                            gld_efficiency             Global Memory Load Efficiency      70.05%      70.05%      70.05%\n",
      "          1                            gst_efficiency            Global Memory Store Efficiency     100.00%     100.00%     100.00%\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics gld_efficiency,gst_efficiency python 3_1_2_convolve_strided_access.py --mode benchmark n=1 quiet=1 2>&1 | grep -v \"^=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_2mBwUDf87yB",
   "metadata": {
    "id": "_2mBwUDf87yB"
   },
   "source": [
    "# Exercise 3.2. Control flow: how code branching affects the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5jF4RobZ2nDg",
   "metadata": {
    "id": "5jF4RobZ2nDg"
   },
   "source": [
    "Due to SIMT architecture of the CUDA multiprocessors, it is recommended to avoid different paths within the same warp.\n",
    "\n",
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> Flow control instructions (if, switch, do, for, while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nZNBa4eG2_Rj",
   "metadata": {
    "id": "nZNBa4eG2_Rj"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4DwABDbg3BLR",
   "metadata": {
    "id": "4DwABDbg3BLR"
   },
   "source": [
    "Let's implement the following function:\n",
    "\n",
    "```\n",
    "y[i] = r[i]*a[i] + b[i]\n",
    "```\n",
    "\n",
    "where `r[i] = i mod 8`.\n",
    "\n",
    "We can implement it in one of the two ways:\n",
    "1. directly by definition (see `add_vectors_mod8_kernel`),\n",
    "2. by doing a sequence of `if ... elif ... elif ... else` blocks (see `add_vectors_mod8_branches_kernel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gEV3cwOb3r8m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793236640,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "gEV3cwOb3r8m",
    "outputId": "9198f154-42ce-4341-9d9e-610ebdd0d8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_2_control_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_control_flow.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_mod8_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    r = i % 8 + 1\n",
    "    result[i] = r*a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_mod8(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_mod8_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_mod8_branches_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    if i % 8 == 0:\n",
    "        result[i] = a[i] + b[i]\n",
    "    elif i % 8 == 1:\n",
    "        result[i] = 2*a[i] + b[i]\n",
    "    elif i % 8 == 2:\n",
    "        result[i] = 3*a[i] + b[i]\n",
    "    elif i % 8 == 3:\n",
    "        result[i] = 4*a[i] + b[i]\n",
    "    elif i % 8 == 4:\n",
    "        result[i] = 5*a[i] + b[i]\n",
    "    elif i % 8 == 5:\n",
    "        result[i] = 6*a[i] + b[i]\n",
    "    elif i % 8 == 6:\n",
    "        result[i] = 7*a[i] + b[i]\n",
    "    elif i % 8 == 7:\n",
    "        result[i] = 8*a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_mod8_branches(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_mod8_branches_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8)\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yag_hEM8CEuc",
   "metadata": {
    "id": "yag_hEM8CEuc"
   },
   "source": [
    "Let's check how much time does it take to execute each of the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3syxy_Dq4Piu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7627,
     "status": "ok",
     "timestamp": 1624793244268,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "3syxy_Dq4Piu",
    "outputId": "649beb56-370d-442f-dfcf-a2aeb1061421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7429== NVPROF is profiling process 7429, command: python 3_2_control_flow.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0136 seconds (+/- 0.0409), median: 0.0090\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0124 seconds (+/- 0.0186), median: 0.0101\n",
      "==7429== Profiling application: python 3_2_control_flow.py\n",
      "==7429== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   50.61%  789.60ms       600  1.3160ms  1.2820ms  2.5836ms  [CUDA memcpy DtoH]\n",
      "                   37.83%  590.28ms       400  1.4757ms  1.3868ms  1.6767ms  [CUDA memcpy HtoD]\n",
      "                    9.54%  148.84ms       100  1.4884ms  1.4873ms  1.4910ms  cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                    2.02%  31.454ms       100  314.54us  312.84us  316.46us  cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_2_control_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a-Tf_qxCBm9h",
   "metadata": {
    "id": "a-Tf_qxCBm9h"
   },
   "source": [
    "Let's also measure `branch_efficiency` metric defined as a:\n",
    "> Ratio of non-divergent branches to total branches expressed as percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "v7htXLWzAkEM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7883,
     "status": "ok",
     "timestamp": 1624793252171,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "v7htXLWzAkEM",
    "outputId": "6bc275a6-ee41-4a07-ed77-59318a7134b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7458== NVPROF is profiling process 7458, command: python 3_2_control_flow.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0166 seconds (+/- 0.0331), median: 0.0121\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0158 seconds (+/- 0.0164), median: 0.0133\n",
      "==7458== Profiling application: python 3_2_control_flow.py\n",
      "==7458== Profiling result:\n",
      "==7458== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce MX250 (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                         branch_efficiency                         Branch Efficiency     100.00%     100.00%     100.00%\n",
      "    Kernel: cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                         branch_efficiency                         Branch Efficiency      58.82%      58.82%      58.82%\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics branch_efficiency python 3_2_control_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7rq1MbBJI0CO",
   "metadata": {
    "id": "7rq1MbBJI0CO"
   },
   "source": [
    "Of course, the above example has been artificially complicated just to show the effect of complex kernel logic on the kernel's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DaX4hxR09G0B",
   "metadata": {
    "id": "DaX4hxR09G0B"
   },
   "source": [
    "# Exercise 3.3. Multiprocessor occupancy: thread block size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pIGIxvK6JG5n",
   "metadata": {
    "id": "pIGIxvK6JG5n"
   },
   "source": [
    "Recall that:\n",
    "> The number of threads per block should be a **multiple of 32 threads**, because this provides optimal computing efficiency and facilitates coalescing.\n",
    "\n",
    "[CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) also gives some other suggestions how to choose the proper number of threads per block:\n",
    "\n",
    "> There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:\n",
    "> 1. Threads per block should be **a multiple of warp size** to avoid wasting computation on under-populated warps and to facilitate coalescing.\n",
    "> 2. A **minimum of 64 threads** per block should be used, and only if there are multiple concurrent blocks per multiprocessor.\n",
    "> 3. Between **128 and 256 threads** per block is a good initial range for experimentation with different block sizes.\n",
    "> 4. Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VToK2NYJRRgk",
   "metadata": {
    "id": "VToK2NYJRRgk"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xq06puf9Q2md",
   "metadata": {
    "id": "xq06puf9Q2md"
   },
   "source": [
    "Let's mesure `add_vectors`' occupancy for a different number of threads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oho6SbZORUFs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793252195,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "oho6SbZORUFs",
    "outputId": "7d26a1bd-05f7-4a21-ebea-ecf4d37d84dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_16.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_16.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 16\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "J58oLJpDRi9k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4435,
     "status": "ok",
     "timestamp": 1624793256631,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "J58oLJpDRi9k",
    "outputId": "9a56b460-21aa-4469-be5f-be53df6240ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7510== NVPROF is profiling process 7510, command: python 3_3_occupancy_16.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0147 seconds (+/- 0.0326), median: 0.0109\n",
      "==7510== Profiling application: python 3_3_occupancy_16.py\n",
      "==7510== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   51.11%  414.19ms       300  1.3806ms  1.2831ms  2.9075ms  [CUDA memcpy DtoH]\n",
      "                   40.67%  329.56ms       200  1.6478ms  1.4194ms  3.6617ms  [CUDA memcpy HtoD]\n",
      "                    8.21%  66.563ms       100  665.63us  653.01us  755.47us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_16.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKyfKBGpWCtR",
   "metadata": {
    "id": "LKyfKBGpWCtR"
   },
   "source": [
    "According to NVIDIA documentation, `achieved_occupancy` measures:\n",
    "> Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "725eQmA6V5Fj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5279,
     "status": "ok",
     "timestamp": 1624793261927,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "725eQmA6V5Fj",
    "outputId": "60ddba76-8a60-472f-be6a-2fbeab3c7939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7546== NVPROF is profiling process 7546, command: python 3_3_occupancy_16.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0156 seconds (+/- 0.0288), median: 0.0123\n",
      "==7546== Profiling application: python 3_3_occupancy_16.py\n",
      "==7546== Profiling result:\n",
      "==7546== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce MX250 (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.331074    0.335876    0.333732\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_16.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "WBy_xzl6Rd0m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793261954,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "WBy_xzl6Rd0m",
    "outputId": "a70eaa06-142d-40a7-85a9-ece016f88db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_256.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_256.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "EYxXfEU8WOWU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8126,
     "status": "ok",
     "timestamp": 1624793270081,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "EYxXfEU8WOWU",
    "outputId": "c7fa80d2-8ed7-471b-a341-3f870249399c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7572== NVPROF is profiling process 7572, command: python 3_3_occupancy_256.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0126 seconds (+/- 0.0319), median: 0.0089\n",
      "==7572== Profiling application: python 3_3_occupancy_256.py\n",
      "==7572== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   55.14%  393.50ms       300  1.3117ms  1.2834ms  1.6101ms  [CUDA memcpy DtoH]\n",
      "                   41.38%  295.29ms       200  1.4764ms  1.4039ms  1.7570ms  [CUDA memcpy HtoD]\n",
      "                    3.48%  24.842ms       100  248.42us  247.81us  254.41us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n",
      "==7600== NVPROF is profiling process 7600, command: python 3_3_occupancy_256.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0165 seconds (+/- 0.0311), median: 0.0123\n",
      "==7600== Profiling application: python 3_3_occupancy_256.py\n",
      "==7600== Profiling result:\n",
      "==7600== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce MX250 (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.890686    0.899663    0.894122\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_256.py\n",
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_256.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "TFUXkREwReV8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1624793270106,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "TFUXkREwReV8",
    "outputId": "b882475c-0cb8-456b-e7b9-4b09d089dfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_1024.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_1024.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 1024\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "CNGVqycVWXyB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8468,
     "status": "ok",
     "timestamp": 1624793278575,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "CNGVqycVWXyB",
    "outputId": "267592da-17a9-449d-fde0-2c8f03ac3910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7626== NVPROF is profiling process 7626, command: python 3_3_occupancy_1024.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0140 seconds (+/- 0.0363), median: 0.0093\n",
      "==7626== Profiling application: python 3_3_occupancy_1024.py\n",
      "==7626== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   55.28%  419.03ms       300  1.3968ms  1.2898ms  2.8975ms  [CUDA memcpy DtoH]\n",
      "                   41.41%  313.87ms       200  1.5693ms  1.3803ms  4.3742ms  [CUDA memcpy HtoD]\n",
      "                    3.32%  25.140ms       100  251.40us  249.25us  273.77us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n",
      "==7653== NVPROF is profiling process 7653, command: python 3_3_occupancy_1024.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0152 seconds (+/- 0.0287), median: 0.0120\n",
      "==7653== Profiling application: python 3_3_occupancy_1024.py\n",
      "==7653== Profiling result:\n",
      "==7653== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce MX250 (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.827707    0.840541    0.833415\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_1024.py\n",
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_1024.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dAsTyfuWmC0",
   "metadata": {
    "id": "1dAsTyfuWmC0"
   },
   "source": [
    "Finding the right number of threads per block requires some experimentation, but generally 128 or 256 threads is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ufa_O1_J9vTI",
   "metadata": {
    "id": "Ufa_O1_J9vTI"
   },
   "source": [
    "# Exercise 3.4. Instruction-level optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GX352UzFpPsX",
   "metadata": {
    "id": "GX352UzFpPsX"
   },
   "source": [
    "This chapter covers the following:\n",
    "\n",
    "1. Impact of the data type selection and conversion on the kernel's performance.\n",
    "2. Floating-point intrinsics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cn_-MQH1Vn4l",
   "metadata": {
    "id": "Cn_-MQH1Vn4l"
   },
   "source": [
    "## Exercise 3.4.1. Data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DUaJzNKup3SX",
   "metadata": {
    "id": "DUaJzNKup3SX"
   },
   "source": [
    "When implementing a CUDA GPU kernel, keep the following in mind:\n",
    "\n",
    "- the choice of the input data type may affect the kernel performance,\n",
    "- data type conversion in kernel implementation may affect kernel performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oRKBDy19qKZI",
   "metadata": {
    "id": "oRKBDy19qKZI"
   },
   "source": [
    "Let's do some comparison of  `float32` and `float64` data, based on an example of a one-dimensional convolution.\n",
    "\n",
    "First, let's recall our baseline implementation of convolution operator.\n",
    "\n",
    "NOTE:\n",
    "- our benchmark function generates `float32` input data,\n",
    "- we used in the kernel implementation the `float32` keyword to enforce the proper data type of `value` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2Mq2y1_fW48S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793278601,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "2Mq2y1_fW48S",
    "outputId": "6f695fdd-f94a-432c-d31f-03ba34bc614e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_4_1_convolve_float32.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_1_convolve_float32.py\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "from gpu_short_course.tests import benchmark_convolve\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(y):\n",
    "        return\n",
    "    M, N = len(x), len(h)\n",
    "    \n",
    "    o = int(math.ceil(N/2)-1)\n",
    "\n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        k = i+o-j\n",
    "        if k >= 0 and k < M:\n",
    "            value += x[k]*h[j]\n",
    "    y[i] = value\n",
    "    \n",
    "\n",
    "def convolve_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block_size = 256\n",
    "    grid_size = math.ceil(len(y)/block_size)\n",
    "    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "benchmark_convolve(convolve_gpu, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6iwhZPo4W8OM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14080,
     "status": "ok",
     "timestamp": 1624793292682,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "6iwhZPo4W8OM",
    "outputId": "d0db9dda-91a2-4dd1-ea20-70c9afdd2e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7679== NVPROF is profiling process 7679, command: python 3_4_1_convolve_float32.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0291 seconds (+/- 0.0366), median: 0.0238\n",
      "==7679== Profiling application: python 3_4_1_convolve_float32.py\n",
      "==7679== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   81.13%  1.88550s       100  18.855ms  17.327ms  29.681ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   12.16%  282.55ms       300  941.84us  1.1200us  2.8355ms  [CUDA memcpy DtoH]\n",
      "                    6.71%  155.99ms       200  779.97us     896ns  3.6081ms  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_4_1_convolve_float32.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kj_TCkJVsHfP",
   "metadata": {
    "id": "kj_TCkJVsHfP"
   },
   "source": [
    "Now let's get rid of the `float32` keyword on line `17` and see if it has any effect on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "Wp7G2hWfW-5y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1624793292711,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "Wp7G2hWfW-5y",
    "outputId": "7cd57113-c7dc-4f8a-e48e-fa30ce71fde0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_4_1_convolve_float32_and_float64.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_1_convolve_float32_and_float64.py\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "from gpu_short_course.tests import benchmark_convolve\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(y):\n",
    "        return\n",
    "    M, N = len(x), len(h)\n",
    "    \n",
    "    o = int(math.ceil(N/2)-1)\n",
    "\n",
    "    value = 0.0\n",
    "    for j in range(N):\n",
    "        k = i+o-j\n",
    "        if k >= 0 and k < M:\n",
    "            value += x[k]*h[j]\n",
    "    y[i] = value\n",
    "    \n",
    "\n",
    "def convolve_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block_size = 256\n",
    "    grid_size = math.ceil(len(y)/block_size)\n",
    "    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "benchmark_convolve(convolve_gpu, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "q3_ezLMZXASn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15556,
     "status": "ok",
     "timestamp": 1624793308268,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "q3_ezLMZXASn",
    "outputId": "57c6c826-a43b-4944-c9e4-f11c2bfd880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7707== NVPROF is profiling process 7707, command: python 3_4_1_convolve_float32_and_float64.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0398 seconds (+/- 0.0367), median: 0.0341\n",
      "==7707== Profiling application: python 3_4_1_convolve_float32_and_float64.py\n",
      "==7707== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   87.04%  2.95362s       100  29.536ms  26.907ms  43.880ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                    8.28%  280.85ms       300  936.16us  1.1200us  2.6610ms  [CUDA memcpy DtoH]\n",
      "                    4.69%  159.02ms       200  795.08us     896ns  3.6370ms  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_4_1_convolve_float32_and_float64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z6c7gjfpXC9v",
   "metadata": {
    "id": "Z6c7gjfpXC9v"
   },
   "source": [
    "The processing time may increase, because 0.0 is a float64, and we are doing promotion from float32 to float64, i.e.:\n",
    "\n",
    "`value += (float64)(x[k]*h[j])`\n",
    "\n",
    "then we downgrade from float64 to float32:\n",
    "\n",
    "`y[i] = (float32)value`.\n",
    "\n",
    "(note: the above may vary between different GPUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sUntEbVrXG6W",
   "metadata": {
    "id": "sUntEbVrXG6W"
   },
   "source": [
    "Lets check what results we will get when we will use only `float64` values in computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "N7jeCX3vXIk3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1624793308295,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "N7jeCX3vXIk3",
    "outputId": "e5d5408f-8938-4e00-c462-05dddd52719b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_4_1_convolve_float64.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_1_convolve_float64.py\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "from gpu_short_course.tests import benchmark_convolve\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(y):\n",
    "        return\n",
    "    M, N = len(x), len(h)\n",
    "    \n",
    "    o = int(math.ceil(N/2)-1)\n",
    "\n",
    "    value = 0.0\n",
    "    for j in range(N):\n",
    "        k = i+o-j\n",
    "        if k >= 0 and k < M:\n",
    "            value += x[k]*h[j]\n",
    "    y[i] = value\n",
    "    \n",
    "\n",
    "def convolve_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block_size = 256\n",
    "    grid_size = math.ceil(len(y)/block_size)\n",
    "    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "benchmark_convolve(convolve_gpu, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "UnnVJp5JXKHg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19407,
     "status": "ok",
     "timestamp": 1624793327703,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "UnnVJp5JXKHg",
    "outputId": "4b15e567-8a6a-44b5-daf8-b24fe7d93496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7735== NVPROF is profiling process 7735, command: python 3_4_1_convolve_float64.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0342 seconds (+/- 0.0381), median: 0.0294\n",
      "==7735== Profiling application: python 3_4_1_convolve_float64.py\n",
      "==7735== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   70.00%  1.97212s       100  19.721ms  17.682ms  29.016ms  cudapy::__main__::convolve_gpu_kernel$241(Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>)\n",
      "                   19.14%  539.18ms       300  1.7973ms  1.1840us  5.2818ms  [CUDA memcpy DtoH]\n",
      "                   10.86%  306.04ms       200  1.5302ms     992ns  4.6555ms  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_4_1_convolve_float64.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484XwMSnugNk",
   "metadata": {
    "id": "484XwMSnugNk"
   },
   "source": [
    "The above results may differ between different GPU cards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eRPoAi2G9yay",
   "metadata": {
    "id": "eRPoAi2G9yay"
   },
   "source": [
    "## Exercise 3.4.2. Floating point intrisincs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A4kVCX0lurgR",
   "metadata": {
    "id": "A4kVCX0lurgR"
   },
   "source": [
    "CUDA Math API provides a set of intrinsic functions, specialized to carry out various calculations. Sometimes, using an intrisinc function explicitly in your code may improve its performance.\n",
    "\n",
    "A complete list of intrinsic functions for CUDA C/C++ is available [here](https://docs.nvidia.com/cuda/cuda-math-api/).\n",
    "\n",
    "A complete list of floating-point intrinsics in Numba is avalable [here](https://numba.pydata.org/numba-doc/latest/cuda-reference/kernel.html#floating-point-intrinsics). \n",
    "\n",
    "Note: At the stage of optimizing the machine code, the compiler may decide to use the intrinsic function (the same or similar), regardless of whether we used it in our implementation or not. Still, if you want to be sure that the intrinsic function is used, we should call it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kl2i6H0cwI41",
   "metadata": {
    "id": "kl2i6H0cwI41"
   },
   "source": [
    "Let's check if using `cuda.fma` intrinsic in our `convolution` gives us any improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0Z5QSCr8XopT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1624793374573,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "0Z5QSCr8XopT",
    "outputId": "40cb571b-f9d4-47e6-da10-fe62ac289d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_4_2_convolve_intrinsics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_4_2_convolve_intrinsics.py\n",
    "import math\n",
    "from numba import cuda, float32\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "import cupy as cp\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(y):\n",
    "        return\n",
    "\n",
    "    M, N = len(x), len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        k = i + o - j\n",
    "        if k >= 0 and k < M:\n",
    "            value += x[k]*h[j]\n",
    "    y[i] = value\n",
    "\n",
    "\n",
    "def convolve(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block_size = 256\n",
    "    grid_size = math.ceil(len(y)/block_size)\n",
    "    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_fma_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(y):\n",
    "        return\n",
    "\n",
    "    M, N = len(x), len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        k = i + o - j\n",
    "        if k >= 0 and k < M:\n",
    "            value = cuda.fma(x[k], h[j], value)\n",
    "    y[i] = value\n",
    "\n",
    "\n",
    "def convolve_fma(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block_size = 256\n",
    "    grid_size = math.ceil(len(y)/block_size)\n",
    "    convolve_fma_gpu_kernel[grid_size, block_size](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_convolve(convolve_fma)\n",
    "gpu_short_course.tests.benchmark_convolve(convolve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "JMjAyrbSY9gP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8993,
     "status": "ok",
     "timestamp": 1624793385776,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "JMjAyrbSY9gP",
    "outputId": "27684aaa-21b9-4c07-b20c-5eaae86da32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==7764== NVPROF is profiling process 7764, command: python 3_4_2_convolve_intrinsics.py\n",
      "GPU:0: b'GeForce MX250'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0282 seconds (+/- 0.0360), median: 0.0241\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0263 seconds (+/- 0.0114), median: 0.0246\n",
      "==7764== Profiling application: python 3_4_2_convolve_intrinsics.py\n",
      "==7764== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   41.20%  1.89466s       100  18.947ms  17.413ms  23.234ms  cudapy::__main__::convolve_gpu_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   40.47%  1.86077s       100  18.608ms  17.414ms  28.650ms  cudapy::__main__::convolve_fma_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   11.75%  540.38ms       600  900.64us  1.0880us  2.7701ms  [CUDA memcpy DtoH]\n",
      "                    6.58%  302.60ms       400  756.49us     896ns  2.6016ms  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "!nvprof --trace gpu python 3_4_2_convolve_intrinsics.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_Performance_guidelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
