{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outer-promise",
   "metadata": {
    "id": "outer-promise"
   },
   "source": [
    "# Lecture 3. Performance guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J7GiIP54uVnu",
   "metadata": {
    "id": "J7GiIP54uVnu"
   },
   "source": [
    "In this notebook we will describe the most important guidelines when programming code for NVIDIA GPU cards.\n",
    "\n",
    "We will consider the following aspects of optimizing CUDA kernels:\n",
    "- memory access patterns: *memory coalescing* for the best throughput,\n",
    "- control flow: how code branching affects the performance,\n",
    "- multiprocessor occupancy: experimenting with different block sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K7TzNH4sUfFd",
   "metadata": {
    "id": "K7TzNH4sUfFd"
   },
   "source": [
    "## Exercise 3.1. Memory access patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q2u5NmF997ls",
   "metadata": {
    "id": "Q2u5NmF997ls"
   },
   "source": [
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.\n",
    "\n",
    "This means that the number of useful memory accesses done by our kernel, and thus its performance, largely depends on the memory access pattern it does.\n",
    "\n",
    "Our goal is to implement a GPU kernel that only loads **useful** data from global memory that will then be used in the calculations. We can achieve this with **coalesced memory accesses**.\n",
    "\n",
    "To achive coalesced memory accesses in our kernel, we need to meet the following conditions:\n",
    "- the number of threads per block is a multiple of 32 threads,\n",
    "- sequential threads in a warp access memory that is sequential.\n",
    "\n",
    "\n",
    "For example, our baseline `add_vectors_gpu` and `convolve_gpu` implementations satisfy the above conditions:\n",
    "- the number of threads per block was equal 256,\n",
    "- adjacent threads were reading adjacent memory areas, e.g. thread `i` read the `a[i]` and `b[i]`, and thread `i+1` read `a[i+1]` and `b[i+1]`.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hVK75cukFY1P",
   "metadata": {
    "id": "hVK75cukFY1P"
   },
   "source": [
    "We will discuss below what are the reasons for both of the conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pn3fT7ViAlSV",
   "metadata": {
    "id": "pn3fT7ViAlSV"
   },
   "source": [
    "### Exercise 3.1.1. Impact of misaligned accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7g2ZEr-MERds",
   "metadata": {
    "id": "7g2ZEr-MERds"
   },
   "source": [
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qmYr7eNmEpn7",
   "metadata": {
    "id": "qmYr7eNmEpn7"
   },
   "source": [
    "Recall that warp reads global memory by using a sequence of **32-byte** segments transactions.\n",
    "\n",
    "Note that, according to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> Memory allocated through the CUDA Runtime API (...) is guaranteed to be aligned to at least 256 bytes.\n",
    "\n",
    "This means that if the block size is a multiple of the warp size, each block will load only its own data chunk from memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2CgD_Nl-ONbm",
   "metadata": {
    "id": "2CgD_Nl-ONbm"
   },
   "source": [
    "#### Example\n",
    "\n",
    "As an example, we will consider here:\n",
    "- `add_vectors_gpu` function,\n",
    "- block size = 13. \n",
    "\n",
    "Now let's take a look what memory accesses will be performed by thread block 0, 1, etc.\n",
    "\n",
    "**Block 0**\n",
    "\n",
    "- reads memory area [0, 52), size: 52 bytes (13 x 4-byte floats)\n",
    "\n",
    "```\n",
    "[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n",
    "[       block 0 data (52 bytes)       ]\n",
    "```\n",
    "\n",
    "- This requires 2 x 32-byte transfers. \n",
    "- However only 52 bytes will be used (81%). \n",
    "\n",
    "**Block 1**\n",
    "\n",
    "- Reads memory area [52, 104), size: 52 bytes.\n",
    "```\n",
    "[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n",
    "                                       [       block 1 data (52 bytes)       ]\n",
    "```\n",
    "- This requires 3 x 32-byte transfers.\n",
    "- However only 52 bytes will be used (54%). \n",
    "\n",
    "\n",
    "**And so on...**\n",
    "\n",
    "\n",
    "As we can see, we are transfer a large amount of (theoretically) useless data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moOL-8cYJT12",
   "metadata": {
    "id": "moOL-8cYJT12"
   },
   "source": [
    "Let's see if there is any observable performance difference between scripts using 256 and 261 threads in a **block**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qrPeRyL4hIRu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1624793206802,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "qrPeRyL4hIRu",
    "outputId": "faf4d8fe-e24e-4248-b416-0dc718771bb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_1_1_aligned.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_1_aligned.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rkdpd78-iBQ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4045,
     "status": "ok",
     "timestamp": 1624793210846,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "rkdpd78-iBQ6",
    "outputId": "5348af40-01ae-4be3-c544-34c6394e274c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23182== NVPROF is profiling process 23182, command: python 3_1_1_aligned.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0088 seconds (+/- 0.0374), median: 0.0040\n",
      "==23182== Profiling application: python 3_1_1_aligned.py\n",
      "==23182== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   60.56%  115.60ms       300  385.35us  338.89us  900.86us  [CUDA memcpy DtoH]\n",
      "                   36.77%  70.185ms       200  350.92us  346.22us  395.60us  [CUDA memcpy HtoD]\n",
      "                    2.67%  5.0905ms       100  50.905us  49.506us  52.609us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_1_aligned.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yZNAOdG8h-xB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1624793210895,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "yZNAOdG8h-xB",
    "outputId": "5f0980cd-902e-4d75-fecb-45f8af3e07d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_1_1_misaligned.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_1_misaligned.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 261\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "G5TINGpYldmo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1624793214953,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "G5TINGpYldmo",
    "outputId": "7f5cb1d0-006d-44a5-a63d-78d75caab994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23268== NVPROF is profiling process 23268, command: python 3_1_1_misaligned.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0086 seconds (+/- 0.0340), median: 0.0040\n",
      "==23268== Profiling application: python 3_1_1_misaligned.py\n",
      "==23268== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   60.54%  116.19ms       300  387.30us  339.34us  711.03us  [CUDA memcpy DtoH]\n",
      "                   36.79%  70.600ms       200  353.00us  346.03us  406.54us  [CUDA memcpy HtoD]\n",
      "                    2.67%  5.1324ms       100  51.324us  50.113us  52.833us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_1_misaligned.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kdy0Y21Tlvij",
   "metadata": {
    "id": "Kdy0Y21Tlvij"
   },
   "source": [
    "The difference in performance of the aligned and misaligned versions is rather minor (on some devices even negligible).\n",
    "\n",
    "Why? \n",
    "\n",
    "In this particular case, adjacent warps **reuse the cached data** their neighbors fetched. \n",
    "\n",
    "Anyway, setting a block size to a multiple of warp size, might be a good rule of thumb: it facilitates coalescing, and (as we will discuss later), helps to avoid wasting multiprocessor computation time on under-populated warps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WVH-4tO1ApIv",
   "metadata": {
    "id": "WVH-4tO1ApIv"
   },
   "source": [
    "### Exercise 3.1.2. Impact of strided accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QAD5qaCi8yhV",
   "metadata": {
    "id": "QAD5qaCi8yhV"
   },
   "source": [
    "A non-unit-strided global memory accesses may impact effective memory bandwidth. \n",
    "\n",
    "We say that GPU kernel performs a unit-strided memory access, if threads with successive identifiers read the data from successive memory areas, in other words, the following access pattern is respected:\n",
    "\n",
    "```\n",
    "x = data[(some custom offset) + threadIdx.x]\n",
    "```\n",
    "\n",
    "When using the data access notation for a multidimensional array, make sure that the last axis is addressed using `threadIdx.x`:\n",
    "\n",
    "```\n",
    "x = data[(other dimensions...), threadIdx.x]\n",
    "```\n",
    "\n",
    "\n",
    "The degradation in performance can be especially apparent when working with multidimensional arrays - the choice of the axis, along which choosing a specific operation is performed, can affect effective bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urpbVefb9WPH",
   "metadata": {
    "id": "urpbVefb9WPH"
   },
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-activation",
   "metadata": {
    "id": "driving-activation"
   },
   "source": [
    "Let's consider a 1D convolution along one of the axes of a 2D array.\n",
    "\n",
    "```\n",
    "         axis 1\n",
    "     ---------------\n",
    "x = [[0,  1,  2,  3], |\n",
    "     [4,  5,  6,  7], | axis 0\n",
    "     [8,  9, 10, 11]] |\n",
    "\n",
    "h = [1, 1]\n",
    "\n",
    "```\n",
    "\n",
    "NumPy stores arrays in row-major order, so the above array is actually kept in computer's memory as a following 1D array:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-packaging",
   "metadata": {
    "id": "possible-packaging"
   },
   "source": [
    "Let's consider doing convolution along axis 0 and 1.\n",
    "\n",
    "**Convolve along axis 1**:\n",
    "\n",
    "```\n",
    "         axis 1\n",
    "     ---------------\n",
    "x = [[0,  1,  2,  3]  *  [1, 1] = [0,  1,  3,  5] \n",
    "     [4,  5,  6,  7], *  [1, 1] = [4,  9, 11, 13]\n",
    "     [8,  9, 10, 11]] *  [1, 1] = [8, 17, 19, 21]\n",
    "```\n",
    "\n",
    "For the first output row:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n",
    "    [1]      y = [0]\n",
    "    [1, 1]       [1]\n",
    "       [1, 1]    [3]\n",
    "          [1, 1] [5]\n",
    "\n",
    "```\n",
    "\n",
    "`y[0]` `y[1]`, `y[2]` and `y[3]` are computed by threads with `threadIdx.x` equal `0`, `1`, `2` and `3`, respectively.\n",
    "\n",
    "**Convolve along axis 0**:\n",
    "\n",
    "```\n",
    "x = [[0,  1,   2,   3]  | \n",
    "     [4,  5,   6,   7], | axis 0\n",
    "     [8,  9,  10,  11]] | \n",
    "      *   *    *    *\n",
    "     [1] [1]  [1]  [1]\n",
    "     [1] [1]  [1]  [1]\n",
    "   y  =   =    =    =\n",
    "    [ 0] [ 1] [ 2] [ 3]\n",
    "    [ 4] [ 6] [ 8] [10]\n",
    "    [12] [14] [16] [18]\n",
    "```\n",
    "\n",
    "For the first output column:\n",
    "\n",
    "```\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n",
    "    [1]                                    y = [ 0]\n",
    "    [1,          1]                            [ 4]\n",
    "                [1,         1]                 [12]\n",
    "```\n",
    "\n",
    "`y[0]` `y[1]`, and `y[2]` are computed by threads with `threadIdx.x` equal `0`, `1` and `2` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qsmm48sLaOFB",
   "metadata": {
    "id": "qsmm48sLaOFB"
   },
   "source": [
    "As we can see in the above example, the stride is much larger for the convolution along axis 0. Will it impact the bandwidth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adequate-experiment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1624793215022,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "adequate-experiment",
    "outputId": "b77422d7-92af-4e7a-bd05-b2288af55e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_1_2_convolve_strided_access.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_1_2_convolve_strided_access.py\n",
    "import math\n",
    "import numpy as np\n",
    "from numba import cuda, float32\n",
    "import cupy as cp\n",
    "import gpu_short_course\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_axis0_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    N = len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    HEIGHT = x.shape[0]\n",
    "    WIDTH = x.shape[1]\n",
    "    if i >= HEIGHT or j >= WIDTH:\n",
    "        return\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for k in range(N):\n",
    "        l = i + o - k\n",
    "        if l >= 0 and l < HEIGHT:\n",
    "\n",
    "            ## --- Get data along the second (1) axis.\n",
    "            value += x[l, j] * h[k]\n",
    "            \n",
    "    y[i, j] = value\n",
    "    \n",
    "    \n",
    "def convolve_axis0_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block = (32, 32)\n",
    "    height, width = x.shape\n",
    "    block_h, block_w = block\n",
    "    grid = (math.ceil(width/block_w), \n",
    "            math.ceil(height/block_h))\n",
    "    convolve_axis0_gpu_kernel[grid, block](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_axis1_gpu_kernel(y, x, h):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n",
    "\n",
    "    N = len(h)\n",
    "    o = int(math.ceil(N/2)-1)\n",
    "    \n",
    "    HEIGHT = x.shape[0]\n",
    "    WIDTH = x.shape[1]\n",
    "    \n",
    "    if i >= WIDTH or j >= HEIGHT:\n",
    "        return\n",
    "    \n",
    "    value = float32(0.0)\n",
    "    for k in range(N):\n",
    "        l = i + o - k\n",
    "        if l >= 0 and l < WIDTH:\n",
    "            \n",
    "            ## --- Get data along the first (0) axis.\n",
    "            value += x[j, l]*h[k]\n",
    "            \n",
    "    y[j, i] = value\n",
    "    \n",
    "    \n",
    "def convolve_axis1_gpu(x, h):\n",
    "    y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    block = (32, 32)\n",
    "    height, width = x.shape\n",
    "    block_h, block_w = block\n",
    "    grid = (math.ceil(width/block_w), \n",
    "            math.ceil(height/block_h))\n",
    "    convolve_axis1_gpu_kernel[grid, block](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.run_convolve_2d_input(convolve_axis0_gpu, axis=0)\n",
    "gpu_short_course.run_convolve_2d_input(convolve_axis1_gpu, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rising-tuning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1166,
     "status": "ok",
     "timestamp": 1624793216189,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "rising-tuning",
    "outputId": "4c74105d-4aab-418c-a095-a948386b1fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "All tests passed.\n",
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "! python 3_1_2_convolve_strided_access.py --mode test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31NCkiJAy0OJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14117,
     "status": "ok",
     "timestamp": 1624793230308,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "31NCkiJAy0OJ",
    "outputId": "d1eb0750-a9f8-47f4-96c1-627475843530"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23411== NVPROF is profiling process 23411, command: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking, please wait...\n",
      "Benchmarking, please wait...\n",
      "==23411== Profiling application: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n",
      "==23411== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   65.28%  1.59769s       100  15.977ms  15.617ms  18.364ms  cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   25.61%  626.83ms       100  6.2683ms  6.1722ms  6.3813ms  cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                    6.24%  152.66ms       600  254.44us  1.5360us  850.36us  [CUDA memcpy DtoH]\n",
      "                    2.87%  70.183ms       400  175.46us  1.0880us  607.57us  [CUDA memcpy HtoD]\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CTgHQiOX1-vG",
   "metadata": {
    "id": "CTgHQiOX1-vG"
   },
   "source": [
    "On my GPU (Nvidia GeForce MX250), convolution along axis 1 takes much less time than along axis 0.\n",
    "\n",
    "We can use profiler metrics to verify what memory access efficiency for both cases we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "civilian-constant",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6289,
     "status": "ok",
     "timestamp": 1624793236614,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "civilian-constant",
    "outputId": "31917e38-75cb-4083-a866-1e82a46a3cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking, please wait...\n",
      "Benchmarking, please wait...\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce GTX TITAN X (0)\"\n",
      "    Kernel: cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "          1                            gld_efficiency             Global Memory Load Efficiency      12.50%      12.50%      12.50%\n",
      "          1                            gst_efficiency            Global Memory Store Efficiency      12.50%      12.50%      12.50%\n",
      "    Kernel: cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "          1                            gld_efficiency             Global Memory Load Efficiency      70.05%      70.05%      70.05%\n",
      "          1                            gst_efficiency            Global Memory Store Efficiency     100.00%     100.00%     100.00%\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics gld_efficiency,gst_efficiency python 3_1_2_convolve_strided_access.py --mode benchmark n=1 quiet=1 2>&1 | grep -v \"^=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_2mBwUDf87yB",
   "metadata": {
    "id": "_2mBwUDf87yB"
   },
   "source": [
    "# Exercise 3.2. Control flow: how code branching affects the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5jF4RobZ2nDg",
   "metadata": {
    "id": "5jF4RobZ2nDg"
   },
   "source": [
    "Due to SIMT architecture of the CUDA multiprocessors, it is recommended to avoid different paths within the same warp.\n",
    "\n",
    "According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n",
    "> Flow control instructions (if, switch, do, for, while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nZNBa4eG2_Rj",
   "metadata": {
    "id": "nZNBa4eG2_Rj"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4DwABDbg3BLR",
   "metadata": {
    "id": "4DwABDbg3BLR"
   },
   "source": [
    "Let's implement the following function:\n",
    "\n",
    "```\n",
    "y[i] = r[i]*a[i] + b[i]\n",
    "```\n",
    "\n",
    "where `r[i] = i mod 8`.\n",
    "\n",
    "We can implement it in one of the two ways:\n",
    "1. directly by definition (see `add_vectors_mod8_kernel`),\n",
    "2. by doing a sequence of `if ... elif ... elif ... else` blocks (see `add_vectors_mod8_branches_kernel`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "gEV3cwOb3r8m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793236640,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "gEV3cwOb3r8m",
    "outputId": "9198f154-42ce-4341-9d9e-610ebdd0d8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_2_control_flow.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_2_control_flow.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_mod8_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    r = i % 8 + 1\n",
    "    result[i] = r*a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_mod8(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_mod8_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_mod8_branches_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    if i % 8 == 0:\n",
    "        result[i] = a[i] + b[i]\n",
    "    elif i % 8 == 1:\n",
    "        result[i] = 2*a[i] + b[i]\n",
    "    elif i % 8 == 2:\n",
    "        result[i] = 3*a[i] + b[i]\n",
    "    elif i % 8 == 3:\n",
    "        result[i] = 4*a[i] + b[i]\n",
    "    elif i % 8 == 4:\n",
    "        result[i] = 5*a[i] + b[i]\n",
    "    elif i % 8 == 5:\n",
    "        result[i] = 6*a[i] + b[i]\n",
    "    elif i % 8 == 6:\n",
    "        result[i] = 7*a[i] + b[i]\n",
    "    elif i % 8 == 7:\n",
    "        result[i] = 8*a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_mod8_branches(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_mod8_branches_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8)\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yag_hEM8CEuc",
   "metadata": {
    "id": "yag_hEM8CEuc"
   },
   "source": [
    "Let's check how much time does it take to execute each of the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3syxy_Dq4Piu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7627,
     "status": "ok",
     "timestamp": 1624793244268,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "3syxy_Dq4Piu",
    "outputId": "649beb56-370d-442f-dfcf-a2aeb1061421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23580== NVPROF is profiling process 23580, command: python 3_2_control_flow.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0092 seconds (+/- 0.0403), median: 0.0041\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0076 seconds (+/- 0.0230), median: 0.0042\n",
      "==23580== Profiling application: python 3_2_control_flow.py\n",
      "==23580== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   56.09%  232.18ms       600  386.96us  339.79us  1.1761ms  [CUDA memcpy DtoH]\n",
      "                   34.01%  140.76ms       400  351.90us  346.22us  607.63us  [CUDA memcpy HtoD]\n",
      "                    8.20%  33.932ms       100  339.32us  336.97us  341.90us  cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                    1.70%  7.0310ms       100  70.310us  69.762us  72.482us  cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_2_control_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a-Tf_qxCBm9h",
   "metadata": {
    "id": "a-Tf_qxCBm9h"
   },
   "source": [
    "Let's also measure `branch_efficiency` metric defined as a:\n",
    "> Ratio of non-divergent branches to total branches expressed as percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "v7htXLWzAkEM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7883,
     "status": "ok",
     "timestamp": 1624793252171,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "v7htXLWzAkEM",
    "outputId": "6bc275a6-ee41-4a07-ed77-59318a7134b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23664== NVPROF is profiling process 23664, command: python 3_2_control_flow.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0184 seconds (+/- 0.0313), median: 0.0142\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0173 seconds (+/- 0.0192), median: 0.0142\n",
      "==23664== Profiling application: python 3_2_control_flow.py\n",
      "==23664== Profiling result:\n",
      "==23664== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce GTX TITAN X (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                         branch_efficiency                         Branch Efficiency     100.00%     100.00%     100.00%\n",
      "    Kernel: cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                         branch_efficiency                         Branch Efficiency      58.82%      58.82%      58.82%\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics branch_efficiency python 3_2_control_flow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7rq1MbBJI0CO",
   "metadata": {
    "id": "7rq1MbBJI0CO"
   },
   "source": [
    "Of course, the above example has been artificially complicated just to show the effect of complex kernel logic on the kernel's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DaX4hxR09G0B",
   "metadata": {
    "id": "DaX4hxR09G0B"
   },
   "source": [
    "# Exercise 3.3. Multiprocessor occupancy: thread block size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pIGIxvK6JG5n",
   "metadata": {
    "id": "pIGIxvK6JG5n"
   },
   "source": [
    "Recall that:\n",
    "> The number of threads per block should be a **multiple of 32 threads**, because this provides optimal computing efficiency and facilitates coalescing.\n",
    "\n",
    "[CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) also gives some other suggestions how to choose the proper number of threads per block:\n",
    "\n",
    "> There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:\n",
    "> 1. Threads per block should be **a multiple of warp size** to avoid wasting computation on under-populated warps and to facilitate coalescing.\n",
    "> 2. A **minimum of 64 threads** per block should be used, and only if there are multiple concurrent blocks per multiprocessor.\n",
    "> 3. Between **128 and 256 threads** per block is a good initial range for experimentation with different block sizes.\n",
    "> 4. Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VToK2NYJRRgk",
   "metadata": {
    "id": "VToK2NYJRRgk"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xq06puf9Q2md",
   "metadata": {
    "id": "xq06puf9Q2md"
   },
   "source": [
    "Let's mesure `add_vectors`' occupancy for a different number of threads:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "oho6SbZORUFs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793252195,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "oho6SbZORUFs",
    "outputId": "7d26a1bd-05f7-4a21-ebea-ecf4d37d84dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_16.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_16.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 16\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "J58oLJpDRi9k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4435,
     "status": "ok",
     "timestamp": 1624793256631,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "J58oLJpDRi9k",
    "outputId": "9a56b460-21aa-4469-be5f-be53df6240ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23747== NVPROF is profiling process 23747, command: python 3_3_occupancy_16.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0085 seconds (+/- 0.0320), median: 0.0041\n",
      "==23747== Profiling application: python 3_3_occupancy_16.py\n",
      "==23747== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   57.99%  117.13ms       300  390.42us  340.68us  1.0115ms  [CUDA memcpy DtoH]\n",
      "                   34.72%  70.120ms       200  350.60us  345.99us  391.11us  [CUDA memcpy HtoD]\n",
      "                    7.29%  14.717ms       100  147.17us  147.01us  148.68us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_16.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LKyfKBGpWCtR",
   "metadata": {
    "id": "LKyfKBGpWCtR"
   },
   "source": [
    "According to NVIDIA documentation, `achieved_occupancy` measures:\n",
    "> Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725eQmA6V5Fj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5279,
     "status": "ok",
     "timestamp": 1624793261927,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "725eQmA6V5Fj",
    "outputId": "60ddba76-8a60-472f-be6a-2fbeab3c7939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23833== NVPROF is profiling process 23833, command: python 3_3_occupancy_16.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0215 seconds (+/- 0.0311), median: 0.0172\n",
      "==23833== Profiling application: python 3_3_occupancy_16.py\n",
      "==23833== Profiling result:\n",
      "==23833== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce GTX TITAN X (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.184555    0.189857    0.186089\n"
     ]
    }
   ],
   "source": [
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_16.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WBy_xzl6Rd0m",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624793261954,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "WBy_xzl6Rd0m",
    "outputId": "a70eaa06-142d-40a7-85a9-ece016f88db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_256.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_256.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 256\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "EYxXfEU8WOWU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8126,
     "status": "ok",
     "timestamp": 1624793270081,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "EYxXfEU8WOWU",
    "outputId": "c7fa80d2-8ed7-471b-a341-3f870249399c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==23917== NVPROF is profiling process 23917, command: python 3_3_occupancy_256.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0086 seconds (+/- 0.0346), median: 0.0041\n",
      "==23917== Profiling application: python 3_3_occupancy_256.py\n",
      "==23917== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   60.73%  116.27ms       300  387.56us  335.31us  1.1607ms  [CUDA memcpy DtoH]\n",
      "                   36.62%  70.107ms       200  350.53us  345.90us  395.50us  [CUDA memcpy HtoD]\n",
      "                    2.65%  5.0808ms       100  50.807us  49.250us  54.273us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n",
      "==24010== NVPROF is profiling process 24010, command: python 3_3_occupancy_256.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0208 seconds (+/- 0.0289), median: 0.0168\n",
      "==24010== Profiling application: python 3_3_occupancy_256.py\n",
      "==24010== Profiling result:\n",
      "==24010== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce GTX TITAN X (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.869043    0.890883    0.879323\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_256.py\n",
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_256.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "TFUXkREwReV8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1624793270106,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "TFUXkREwReV8",
    "outputId": "b882475c-0cb8-456b-e7b9-4b09d089dfc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 3_3_occupancy_1024.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 3_3_occupancy_1024.py\n",
    "\n",
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "import gpu_short_course.tests\n",
    "\n",
    "block_size = 1024\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def add_vectors_kernel(result, a, b):\n",
    "    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n",
    "    if i >= len(result):\n",
    "        return\n",
    "    result[i] = a[i] + b[i]\n",
    "\n",
    "\n",
    "def add_vectors_gpu(a, b):\n",
    "    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n",
    "    grid_size = math.ceil(len(a)/block_size)\n",
    "    add_vectors_kernel[grid_size, block_size](result, a, b)\n",
    "    return result.copy_to_host()\n",
    "\n",
    "gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "CNGVqycVWXyB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8468,
     "status": "ok",
     "timestamp": 1624793278575,
     "user": {
      "displayName": "Piotr Jarosik",
      "photoUrl": "",
      "userId": "07939774369485222074"
     },
     "user_tz": -120
    },
    "id": "CNGVqycVWXyB",
    "outputId": "267592da-17a9-449d-fde0-2c8f03ac3910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==24096== NVPROF is profiling process 24096, command: python 3_3_occupancy_1024.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0086 seconds (+/- 0.0350), median: 0.0040\n",
      "==24096== Profiling application: python 3_3_occupancy_1024.py\n",
      "==24096== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   60.54%  115.47ms       300  384.89us  337.51us  1.0125ms  [CUDA memcpy DtoH]\n",
      "                   36.81%  70.213ms       200  351.07us  346.06us  404.56us  [CUDA memcpy HtoD]\n",
      "                    2.65%  5.0476ms       100  50.475us  49.505us  51.874us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "No API activities were profiled.\n",
      "==24180== NVPROF is profiling process 24180, command: python 3_3_occupancy_1024.py\n",
      "GPU:0: b'GeForce GTX TITAN X'\n",
      "Benchmarking the function, please wait...\n",
      "Benchmark result: \n",
      "Average processing time: 0.0213 seconds (+/- 0.0292), median: 0.0172\n",
      "==24180== Profiling application: python 3_3_occupancy_1024.py\n",
      "==24180== Profiling result:\n",
      "==24180== Metric result:\n",
      "Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n",
      "Device \"GeForce GTX TITAN X (0)\"\n",
      "    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "        100                        achieved_occupancy                        Achieved Occupancy    0.817932    0.848587    0.832285\n"
     ]
    }
   ],
   "source": [
    "! nvprof --trace gpu python 3_3_occupancy_1024.py\n",
    "! nvprof --metrics achieved_occupancy python 3_3_occupancy_1024.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dAsTyfuWmC0",
   "metadata": {
    "id": "1dAsTyfuWmC0"
   },
   "source": [
    "Finding the right number of threads per block requires some experimentation, but generally 128 or 256 threads is a good starting point."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3_Performance_guidelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
